{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans1\n",
    "Bagging (Bootstrap Aggregating) reduces overfitting in decision trees by creating an ensemble of trees, each trained on a different subset of the data. The key mechanisms are:\n",
    "\n",
    "Bootstrapped Samples: Bagging generates multiple bootstrap samples (random samples with replacement) from the original dataset. Each decision tree in the ensemble is trained on one of these bootstrap samples.\n",
    "\n",
    "Variability and Decorrelation: The random sampling introduces variability in the training data for each tree, leading to differences in the learned patterns. This decorrelation among the trees helps reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans2\n",
    "Advantages:\n",
    "\n",
    "Diversity: Using different types of base learners increases the diversity of the ensemble, enhancing its overall performance.\n",
    "Robustness: Ensemble methods are often less sensitive to the choice of individual models, making them more robust.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Complexity: Using diverse base learners may increase the complexity of the ensemble, making it harder to interpret.\n",
    "Computational Cost: Ensembles with complex base learners may require more computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans3\n",
    "Low Bias, High Variance Base Learner: If the base learner has low bias but high variance (e.g., a deep decision tree), bagging can significantly reduce variance, making the overall ensemble more robust.\n",
    "\n",
    "High Bias, Low Variance Base Learner: If the base learner has high bias but low variance (e.g., a shallow decision tree), bagging might not provide as much benefit in terms of variance reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans4\n",
    "Yes, bagging can be used for both classification and regression tasks.\n",
    "\n",
    "Classification: In classification, bagging typically involves training decision trees on different bootstrap samples and combining their predictions through majority voting. The final prediction is the class that receives the most votes.\n",
    "\n",
    "Regression: In regression, bagging involves training decision trees on different bootstrap samples and averaging their predictions to obtain the final regression output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans5\n",
    "The ensemble size in bagging refers to the number of base models (e.g., decision trees) in the ensemble. The role of ensemble size is to balance improvements in performance with computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans6\n",
    "Example: Random Forest for Image Classification\n",
    "\n",
    "A real-world application of bagging is seen in image classification using a Random Forest. In this scenario:\n",
    "\n",
    "Base Learners: Decision trees are used as base learners.\n",
    "Ensemble Size: A large ensemble of decision trees is trained, each on a different subset of the training images."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
