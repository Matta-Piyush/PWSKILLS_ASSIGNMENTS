{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans1\n",
    "Forward propagation is the process of transmitting input data through the neural network to generate predictions or outputs. It involves computing the weighted sum of inputs, applying activation functions, and passing the information layer by layer through the network until the output layer is reached. The primary purpose is to make predictions based on the current weights and biases of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans2\n",
    "In a single-layer feedforward neural network, the forward propagation can be expressed mathematically as follows:\n",
    "\n",
    "Output=f(Weighted Sum+Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans3\n",
    "Activation functions introduce non-linearity to the network by applying a transformation to the weighted sum of inputs. Common activation functions include sigmoid, tanh, ReLU, and softmax. They determine whether a neuron should be activated or not, allowing the network to learn complex patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans4\n",
    "Weights: Determine the strength of connections between neurons. They are adjusted during training to minimize the difference between predicted and actual outputs.\n",
    "Biases: Act as an offset, allowing the model to learn when all input features are zero. Like weights, biases are adjusted during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans5\n",
    "The softmax function is applied to the output layer in multi-class classification problems. It converts the raw output scores into a probability distribution, ensuring that the sum of probabilities across all classes is equal to 1. This facilitates decision-making by identifying the most likely class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans6\n",
    "Backward propagation, or backpropagation, is the process of updating the weights and biases of a neural network based on the computed error or loss. It involves computing the gradients of the loss with respect to the weights and biases and adjusting them to minimize the error during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans8\n",
    "The chain rule is a fundamental concept in calculus that allows the computation of the derivative of a composite function. In the context of neural networks, the chain rule is applied to calculate the gradients during backpropagation. It states that the derivative of a composite function is the product of the derivatives of its individual functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans9\n",
    "Common challenges include vanishing or exploding gradients, which can affect the convergence of the model. Techniques to address these challenges include using appropriate activation functions (e.g., ReLU to mitigate vanishing gradients), gradient clipping, and using weight initialization strategies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
