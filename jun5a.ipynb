{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans1\n",
    "In artificial neural networks, an activation function is a mathematical operation applied to the output of a neuron, determining whether the neuron should be activated (fire) or not. Activation functions introduce non-linearity to the network, enabling it to learn complex patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans2\n",
    "Common activation functions include:\n",
    "\n",
    "Sigmoid Function (Logistic): Maps inputs to values between 0 and 1.\n",
    "Hyperbolic Tangent (tanh): Similar to the sigmoid but maps inputs to values between -1 and 1.\n",
    "Rectified Linear Unit (ReLU): Outputs the input for positive values, zero for negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans3\n",
    "Activation functions introduce non-linearity, allowing neural networks to model complex relationships in data. They play a crucial role in the training process by enabling backpropagation and the adjustment of weights during optimization. The choice of activation function impacts the network's ability to converge, handle vanishing or exploding gradients, and capture intricate patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans4\n",
    "Advantages:\n",
    "\n",
    "Outputs in a bounded range (0, 1), making it suitable for binary classification.\n",
    "Disadvantages:\n",
    "\n",
    "Susceptible to vanishing gradient problem, slowing down learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans5\n",
    "Unlike the sigmoid, ReLU introduces non-linearity without saturating for positive inputs, mitigating the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans6\n",
    "Benefits of ReLU:\n",
    "\n",
    "Mitigates vanishing gradient problem.\n",
    "Faster convergence due to non-saturation for positive inputs.\n",
    "Simplicity and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans7\n",
    "Leaky ReLU is an extension of ReLU that allows a small negative slope (typically 0.01) for negative inputs. This addresses the issue of neurons becoming inactive (dying ReLU problem) for negative inputs, improving the model's ability to learn from all input ranges and mitigating the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans8\n",
    "Softmax is used in the output layer for multi-class classification problems. It normalizes the raw output scores into a probability distribution, ensuring that the sum of probabilities across classes is equal to 1. It helps in determining the class with the highest probability, facilitating decision-making in multi-class scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans9\n",
    "Comparison with Sigmoid:\n",
    "\n",
    "Like sigmoid, tanh suffers from the vanishing gradient problem.\n",
    "Tanh outputs are zero-centered, which helps in overcoming convergence issues associated with non-zero-centered activation functions like sigmoid."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
