{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans1\n",
    "Gradient Boosting Regression is an ensemble learning technique that combines the predictions of multiple weak learners, usually decision trees, to create a strong predictive model for regression problems. It is an iterative method where each new weak learner corrects the errors made by the combination of existing learners. The optimization process involves minimizing a loss function, often the mean squared error for regression tasks, by updating the model's predictions in the direction of the negative gradient of the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans4\n",
    "In Gradient Boosting, a weak learner is a simple model or a base model that performs slightly better than random chance on the given task. Common choices for weak learners are decision trees with shallow depths (e.g., stumps or small trees). The key characteristic of a weak learner is that it should be better than random guessing but does not need to be highly accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans5\n",
    "The intuition behind the Gradient Boosting algorithm is to build a strong predictive model by combining the predictions of multiple weak learners. Each weak learner corrects the errors made by the previous ones, focusing on the instances that were poorly predicted. The algorithm minimizes a loss function by iteratively updating the model's predictions in the direction of the negative gradient of the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans6\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative fashion. The process involves:\n",
    "\n",
    "Initializing the predictions with the mean (or another initial estimate) of the target variable.\n",
    "Computing the negative gradient (residuals) of the loss function.\n",
    "Fitting a weak learner (e.g., decision tree) to the negative gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans7\n",
    "Initialize Predictions:\n",
    "Compute Negative Gradient (Residuals):\n",
    "Fit Weak Learner to Negative Gradient:\n",
    "Update Predictions:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
