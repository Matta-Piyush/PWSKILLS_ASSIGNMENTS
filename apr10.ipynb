{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans2\n",
    "Bernoulli Naive Bayes: Used for binary data (e.g., presence or absence of a feature). It models features as binary variables (0 or 1).\n",
    "\n",
    "Multinomial Naive Bayes: Used for discrete data where features represent counts or frequencies (e.g., word counts in document classification). It is suitable for data with multiple discrete categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans3\n",
    "In Bernoulli Naive Bayes, missing values are typically handled by ignoring them during the training phase. The assumption is that the absence of a feature (missing value) is equivalent to its non-occurrence.When predicting the class of a new instance with missing values, the Naive Bayes classifier calculates the probabilities based on the available features. If a feature is missing, it is not considered in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans4\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes assumes that the features follow a Gaussian (normal) distribution, and it is commonly used for continuous numerical data.\n",
    "\n",
    "For multi-class classification, the model calculates the likelihood for each class, and the class with the highest likelihood is chosen as the predicted class. Gaussian Naive Bayes is particularly effective when the feature distribution for each class is approximately Gaussian."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
