{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans1\n",
    "The mathematical formula for a linear SVM involves the linear decision function and the margin. Given a feature vector x and a weight vector w, the decision function for a linear SVM is:f(x)=wx+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans2\n",
    "The objective function of a linear SVM aims to maximize the margin between the support vectors of different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans3\n",
    "The kernel trick in SVM allows the algorithm to operate in a higher-dimensional space without explicitly computing the transformations. Instead of computing the dot product in the input space, the kernel function calculates the dot product in the higher-dimensional feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans4\n",
    "Support vectors are the data points that lie on the margin or are misclassified. They play a crucial role in defining the decision boundary and margin. The decision function depends only on these support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ans5\n",
    "The hyperplane in SVM is the decision boundary that separates data points of different classes. The marginal plane is a set of parallel hyperplanes that define the margin. Soft margin and hard margin refer to the flexibility in allowing misclassifications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
